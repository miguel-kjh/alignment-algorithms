{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.30.2)\n",
      "Collecting trl\n",
      "  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: wandb in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.15.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trl) (2.0.1+cu118)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.8/43.8 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: accelerate in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trl) (0.24.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trl) (2.12.0)\n",
      "Collecting tyro>=0.5.11 (from trl)\n",
      "  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.25.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pathtools in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (65.5.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (4.23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: sympy in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.4.0->trl) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.4.0->trl) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.4.0->trl) (3.1.2)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tyro>=0.5.11->trl) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tyro>=0.5.11->trl) (13.4.2)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (2.0.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.4.0->trl) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets->trl) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets->trl) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.4.0->trl) (1.2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Downloading trl-0.9.4-py3-none-any.whl (226 kB)\n",
      "   ---------------------------------------- 0.0/226.7 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 143.4/226.7 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 226.7/226.7 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/9.1 MB 9.4 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.0/9.1 MB 13.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.1/9.1 MB 14.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.1/9.1 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.4/9.1 MB 20.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.6/9.1 MB 21.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.8/9.1 MB 21.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.2/9.1 MB 22.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 22.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.1/9.1 MB 21.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "   ---------------------------------------- 0.0/401.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 401.7/401.7 kB 24.5 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 287.4/287.4 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.0/2.2 MB 31.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 32.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 16.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 15.7 MB/s eta 0:00:00\n",
      "Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "   ---------------------------------------- 0.0/102.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 102.4/102.4 kB ? eta 0:00:00\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: shtab, safetensors, huggingface-hub, tyro, tokenizers, transformers, trl\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.3.1\n",
      "    Uninstalling safetensors-0.3.1:\n",
      "      Successfully uninstalled safetensors-0.3.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.30.2\n",
      "    Uninstalling transformers-4.30.2:\n",
      "      Successfully uninstalled transformers-4.30.2\n",
      "Successfully installed huggingface-hub-0.23.3 safetensors-0.4.3 shtab-1.7.1 tokenizers-0.19.1 transformers-4.41.2 trl-0.9.4 tyro-0.8.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers trl wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"lvwerra/gpt2-imdb\",\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=\"wandb\",\n",
    ")\n",
    "\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiguel_kjh\u001b[0m (\u001b[33msiani-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\code\\alignment-algorithms\\wandb\\run-20240611_152149-8n8ifsl4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/siani-ai/alignment-algorithms/runs/8n8ifsl4' target=\"_blank\">devout-sea-2</a></strong> to <a href='https://wandb.ai/siani-ai/alignment-algorithms' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/siani-ai/alignment-algorithms' target=\"_blank\">https://wandb.ai/siani-ai/alignment-algorithms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/siani-ai/alignment-algorithms/runs/8n8ifsl4' target=\"_blank\">https://wandb.ai/siani-ai/alignment-algorithms/runs/8n8ifsl4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/siani-ai/alignment-algorithms/runs/8n8ifsl4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1f78549ee30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69368aed063f450fb9f3276baefa5b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1681a6fd49d54fb7a129264f0cb03fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(config)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained GPT2 language models (STEP 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e8ccb9316b4206a92f4eb2db5b2584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub\\models--lvwerra--gpt2-imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wejtzoz3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fanciful-firebrand-1</strong> at: <a href='https://wandb.ai/siani-ai/trl/runs/wejtzoz3' target=\"_blank\">https://wandb.ai/siani-ai/trl/runs/wejtzoz3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240611_152809-wejtzoz3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wejtzoz3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af37d3d3273b42cba2df87aeec8754e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\code\\alignment-algorithms\\wandb\\run-20240611_153202-9do82co2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/siani-ai/trl/runs/9do82co2' target=\"_blank\">brisk-wildflower-2</a></strong> to <a href='https://wandb.ai/siani-ai/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/siani-ai/trl' target=\"_blank\">https://wandb.ai/siani-ai/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/siani-ai/trl/runs/9do82co2' target=\"_blank\">https://wandb.ai/siani-ai/trl/runs/9do82co2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT classifier (STEP 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab3a72e62d340268e3307c277037bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub\\models--lvwerra--distilbert-imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8586002228cc4034b5d7fc6ca235a59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683cbf0e09d84afd965cf49e55f562ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d6f8b471a94bbaa0fcef82dafb0f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f97601ca5c4dee84feeffa78702905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27803c08d1e548d5b26844151388309f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': 2.3350484371185303},\n",
       "  {'label': 'POSITIVE', 'score': -2.726576328277588}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this movie was really bad!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': -2.294790267944336},\n",
       "  {'label': 'POSITIVE', 'score': 2.557040214538574}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this movie was really good!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL optimization loop (STEP 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/194 [00:35<39:44, 12.49s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 194/194 [49:41<00:00, 15.37s/it]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(ppo_trainer.dataloader):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now this is</td>\n",
       "      <td>how \"brother\" and \"son\" react when the</td>\n",
       "      <td>a tease, but funny still. I loved it especially</td>\n",
       "      <td>1.350929</td>\n",
       "      <td>2.618836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't think I could</td>\n",
       "      <td>do justice to this selection like I am used t...</td>\n",
       "      <td>rate this movie very well. It's a fun, atmosp...</td>\n",
       "      <td>-1.169286</td>\n",
       "      <td>2.673902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Franco proves, once</td>\n",
       "      <td>again, that no amount of</td>\n",
       "      <td>again, if he can pull</td>\n",
       "      <td>-0.333157</td>\n",
       "      <td>1.091633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My roommate had bought</td>\n",
       "      <td>some splatterbox stuff and he took my pack</td>\n",
       "      <td>this project, and it was such a great family</td>\n",
       "      <td>-0.261592</td>\n",
       "      <td>2.383694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As a</td>\n",
       "      <td>game played in this way, it</td>\n",
       "      <td>'Vanita' the film directly</td>\n",
       "      <td>1.298230</td>\n",
       "      <td>0.079692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rodney Dangerfield</td>\n",
       "      <td>seems to want to please the viewers with his ...</td>\n",
       "      <td>then directed a quite brilliantly hilarious s...</td>\n",
       "      <td>-0.122941</td>\n",
       "      <td>2.784786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This movie was</td>\n",
       "      <td>my \"best\" film at the festival. My question</td>\n",
       "      <td>beautiful and inspiring. This unique cinemati...</td>\n",
       "      <td>2.168100</td>\n",
       "      <td>2.891832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This movie is wonderful</td>\n",
       "      <td>. The story, the</td>\n",
       "      <td>, winning a national Oscar</td>\n",
       "      <td>2.792063</td>\n",
       "      <td>2.818151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Twelve</td>\n",
       "      <td>episodes per series. I can't imagine why this...</td>\n",
       "      <td>is a wonderfully well-crafted film that is a ...</td>\n",
       "      <td>-0.754983</td>\n",
       "      <td>2.941937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Even the first 10 minutes of</td>\n",
       "      <td>the movie should not be a bad thing, more so ...</td>\n",
       "      <td>this movie and I will definitely recommend it...</td>\n",
       "      <td>-0.023760</td>\n",
       "      <td>2.683607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Not that many films</td>\n",
       "      <td>make $3 million at</td>\n",
       "      <td>have Goodman as a</td>\n",
       "      <td>-1.617158</td>\n",
       "      <td>0.237383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>This was a great</td>\n",
       "      <td>movie in every aspect of its day and its phot...</td>\n",
       "      <td>world builder's movement which gave me genuin...</td>\n",
       "      <td>2.900241</td>\n",
       "      <td>2.846445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I liked this</td>\n",
       "      <td>movie. I hope its some sort of Delia, Sharon ...</td>\n",
       "      <td>movie very much. I love bringing for, and thi...</td>\n",
       "      <td>2.224644</td>\n",
       "      <td>2.698321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>it got switched off before</td>\n",
       "      <td>the sequels came out that I still enjoyed the</td>\n",
       "      <td>I saw it. I love this movie.</td>\n",
       "      <td>1.227918</td>\n",
       "      <td>2.651272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I thought this</td>\n",
       "      <td>film was a bad 51-minute worthless</td>\n",
       "      <td>was fantastic film. At first it is</td>\n",
       "      <td>-2.966214</td>\n",
       "      <td>2.350310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Family is about two families</td>\n",
       "      <td>of New York's poor, very intolerant</td>\n",
       "      <td>who choose a beautiful, memorable performance...</td>\n",
       "      <td>-0.521573</td>\n",
       "      <td>2.714729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           query  \\\n",
       "0                    Now this is   \n",
       "1          I don't think I could   \n",
       "2            Franco proves, once   \n",
       "3         My roommate had bought   \n",
       "4                           As a   \n",
       "5             Rodney Dangerfield   \n",
       "6                 This movie was   \n",
       "7        This movie is wonderful   \n",
       "8                         Twelve   \n",
       "9   Even the first 10 minutes of   \n",
       "10           Not that many films   \n",
       "11              This was a great   \n",
       "12                  I liked this   \n",
       "13    it got switched off before   \n",
       "14                I thought this   \n",
       "15  Family is about two families   \n",
       "\n",
       "                                    response (before)  \\\n",
       "0              how \"brother\" and \"son\" react when the   \n",
       "1    do justice to this selection like I am used t...   \n",
       "2                            again, that no amount of   \n",
       "3          some splatterbox stuff and he took my pack   \n",
       "4                         game played in this way, it   \n",
       "5    seems to want to please the viewers with his ...   \n",
       "6         my \"best\" film at the festival. My question   \n",
       "7                                    . The story, the   \n",
       "8    episodes per series. I can't imagine why this...   \n",
       "9    the movie should not be a bad thing, more so ...   \n",
       "10                                 make $3 million at   \n",
       "11   movie in every aspect of its day and its phot...   \n",
       "12   movie. I hope its some sort of Delia, Sharon ...   \n",
       "13      the sequels came out that I still enjoyed the   \n",
       "14                 film was a bad 51-minute worthless   \n",
       "15                of New York's poor, very intolerant   \n",
       "\n",
       "                                     response (after)  rewards (before)  \\\n",
       "0     a tease, but funny still. I loved it especially          1.350929   \n",
       "1    rate this movie very well. It's a fun, atmosp...         -1.169286   \n",
       "2                               again, if he can pull         -0.333157   \n",
       "3        this project, and it was such a great family         -0.261592   \n",
       "4                          'Vanita' the film directly          1.298230   \n",
       "5    then directed a quite brilliantly hilarious s...         -0.122941   \n",
       "6    beautiful and inspiring. This unique cinemati...          2.168100   \n",
       "7                          , winning a national Oscar          2.792063   \n",
       "8    is a wonderfully well-crafted film that is a ...         -0.754983   \n",
       "9    this movie and I will definitely recommend it...         -0.023760   \n",
       "10                                  have Goodman as a         -1.617158   \n",
       "11   world builder's movement which gave me genuin...          2.900241   \n",
       "12   movie very much. I love bringing for, and thi...          2.224644   \n",
       "13                       I saw it. I love this movie.          1.227918   \n",
       "14                 was fantastic film. At first it is         -2.966214   \n",
       "15   who choose a beautiful, memorable performance...         -0.521573   \n",
       "\n",
       "    rewards (after)  \n",
       "0          2.618836  \n",
       "1          2.673902  \n",
       "2          1.091633  \n",
       "3          2.383694  \n",
       "4          0.079692  \n",
       "5          2.784786  \n",
       "6          2.891832  \n",
       "7          2.818151  \n",
       "8          2.941937  \n",
       "9          2.683607  \n",
       "10         0.237383  \n",
       "11         2.846445  \n",
       "12         2.698321  \n",
       "13         2.651272  \n",
       "14         2.350310  \n",
       "15         2.714729  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 16\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    gen_len = output_length_sampler()\n",
    "    output = ref_model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors_ref.append(output)\n",
    "    output = model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
    "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "game_data[\"rewards (before)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "game_data[\"rewards (after)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)    0.386966\n",
       "rewards (after)     2.279158\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "median:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)   -0.073350\n",
       "rewards (after)     2.678755\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"mean:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
    "print()\n",
    "print(\"median:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaca7d2eb8d49cf8d7eb0e049c6fab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('gpt2-imdb-rlhf\\\\tokenizer_config.json',\n",
       " 'gpt2-imdb-rlhf\\\\special_tokens_map.json',\n",
       " 'gpt2-imdb-rlhf\\\\vocab.json',\n",
       " 'gpt2-imdb-rlhf\\\\merges.txt',\n",
       " 'gpt2-imdb-rlhf\\\\added_tokens.json',\n",
       " 'gpt2-imdb-rlhf\\\\tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gpt2-imdb-rlhf\", push_to_hub=True)\n",
    "tokenizer.save_pretrained(\"gpt2-imdb-rlhf\", push_to_hub=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
